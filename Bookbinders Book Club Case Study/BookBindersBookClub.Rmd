---
title: "Book Binders Book Club Case Study"
author: "Matthew Smith"
date: "4/15/2020"
output:
  md_document:
    variant: markdown_github
  html_document:
    df_print: paged
---

```{r setup, include=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Executive Summary


The model that produced the highest prediction accuracy was the support vector machine model. The model scored a 90.96% accuracy on the testing data. The logistic regression model was right behind with a test accuracy of 89.3%. A linear regression model was also run but was never going to be very successful in predicting since the dependent variable in this case was binary. The model was used to look at the significance and importance of the independent variables provided by the Book Binders Book Club.

Both the logistic regression and the support vector machine performed quite well in terms of predicting whether or not the customer purchased The Art History of Florence. While the logistic model was not quite as accurate as the support vector machine, it did provide insight as to which variables were statistically significant. These significant variables ended up being very similar to the linear regression model. Overall, the logistic regression model fared better than the SVM model in terms of predicting true positives, which would lead to more profits.


# The Problem

The task of this particular case study was to isolate the factors that most influenced customers to by the book, The Art History of Florence, from the Book Binders Book Club, and to also develop an accurate model. The overall goal is to be able to target a certain type of customer who is more likely to buy the book. This targeted advertisement will allow for the maximization of profits. This data comes from a subset of the 20,000 customers from Pennsylvania, New York and Ohio who BBBC sent a specially produced brochure in addition to their regular mailing. The BBBC wants to know which type of customers this special marketing campaign works well on, and if the success will translate to other parts of the country, particularly the Midwest.

The Book Binders Book Club has selected three different modeling methods: ordinary linear regression, a logistic regression model, and support vector machines. Since the response variable Choice is binary, the logistic regression and the support vector machine will outperform the linear regression model substantially in terms of predicting whether or not the customer will buy the book. The linear regression model will be used to look at which of the independent variables are statistically significant and what information about customer buying habits can be gained.

Our task defined in this case study is to use these models to build an accurate prediction model as well as uncover insights into which variables are statistically significant in terms of buying the book. This means that there is a balancing act in terms of selecting models. We are not simply looking at the model with the highest prediction accuracy, we also need to be concerned with model complexity. In our case the more complex model, the support vector machine, had the highest prediction accuracy, but offers little insight in terms of interpretability. The logistic regression offered less prediction accuracy but is much more interpretable. Therefore, the model of choice is both the support vector machine and the logistic regression model, chosen on the support vector machine model’s prediction accuracy and the logistic regression model’s interpretability. In the following report, we will explore the methodology used, including model assumptions, selecting hyperparameters for each model, as well as the data itself, which predictors were dropped and why. The report will finish with detailed findings and a conclusion with recommendations.


# Review of Related Literature

In this analysis the support vector machine model achieved the highest prediction accuracy of the test dataset. It also the most complex model performed in this analysis, so interpretability is lacking. This is not the case for the other high performing model, logistic regression. This model is nowhere near as complex and therefore as is quite interpretable.

According to Max Kuhn and Kjell Johnson in their book Applied Predictive Modeling, “SVMs are a class of powerful, highly flexible modeling techniques. The theory behind SVMs was originally developed in the context of classification models.” Since support vector machines have their roots in classification problems, this analysis is the perfect place to use them. The SVM’s highly flexible nature leads to impressive prediction accuracies but is limited in terms of interpretability. This is why we decided to include both the support vector machine model and the logistic regression model as the final models. This allows for the flexibility granted by the SVM for future unseen data as well as the interpretability of the logistic regression model. Combining them together will make the Book Binders Book Club goals, of accurate models as well as which customers they should target, a reality.


# Methodology

For this case study, we compared the results of a linear regression model, a logistic regression model, and a support vector machine model. There are no hyperparameters for the linear and logistic regression. The hyperparameters of a support vector machine are gamma and cost. The final hyperparameters for the support vector machine were a gamma value of 0.04 and a cost value of 0.7. Since the data was pre-processed in the early stage of the analysis, the preprocess function from the caret package was not used as a part of the model building process.

The data was already split into training, test, and prediction. The training set consisted of 1600 variables and the test and prediction sets each had 2300 variables. The prediction set appears to just be the test set without the response variable Choice. It was not used in this analysis. This type of training/test split is a little odd, with the testing set being larger than the training set, it is roughly a 40/60 split, but the accuracies from the models on the large test set were quite good, so it should not be a problem.

The assumptions of the models used in this analysis need to be considered. The assumptions of a linear regression are linearity, homoscedasticity, independence, and normality. The linearity between X and Y assumption for linear regression will not be met since our dependent variable, Choice, is binary. Even when coercing the variable to a numeric type in R this assumption still fails. Another problem is the normality assumption fails for the majority of the predictors. A potential fix is to apply some pre- processing of the data. Here we tried to center and scale the data, as well as apply Box Cox transformations to applicable variables. This helped for a couple of the predictors but not at all for the rest. This, among other things, leads to the poor prediction accuracy produced by our linear regression model. The assumptions of the logistic regression are more laxed in terms of linearity and normality, which works better for the BBBC data.


# Data

The data was extremely clean, which was nice. Both the training and testing datasets had no missing values, so there was no need to drop any observations or impute values. All the variables were listed as numeric in the structure of the data. The dependent variable Choice as well as the Gender variable had to be converted to factors to run the logistic regression and support vector machine models, and then converted back to numeric before running the linear regression.

Only one variable was removed from the dataset, Observations. This was essentially just a row number and therefore served no purpose in the modeling. We next looked at the correlations between the predictors. Normally if a pair of variables had a correlation above 0.8, one of the variables would be dropped from the dataset. Here there was only one instance of highly correlated variables, and that was Last_purhase and First_purchase. There correlation was 0.81, just above the cutoff. Since it was so close and there are only 11 predictors in total, it was decided to leave them both in.

After looking at the histograms of the continuous variables, it appeared that most were heavily skewed. To combat this, we pre-processed the data by using the preprocess function from the caret package. We used the center, scale, and BoxCox methods. The function determined which variables would benefit from each method and applied it to them. Since the data was already split into a training and testing set, the same pre-processing must be applied to both or the accuracies of the models on the testing set will be awful.

One thing to consider is the way the data was obtained from the original 20,000 observation dataset from Pennsylvania, New York, and Ohio. Their response rate was 9.03%. In the test set we are giving 1600 total observations where 400 customers buy the book. That is a response rate of 25%. This is a pretty big difference, which will most likely mean there is some bias in the training set, and in turn in the models in this analysis. These models were tuned and trained with the underlying assumption that the 25% of the observations will have a 1 in the Choice column. This is not representative the overall data from Pennsylvania, New York, and Ohio, and may affect the results when applied to the Midwest dataset. The accuracy rates and significant predictors might be slightly different when applied to a new dataset.


# Findings

This analysis showed that the support vector machine model had the highest accuracy rate of all the models, with an accuracy of 90.96%. Right behind the support vector machine was the logistic regression model, with an accuracy of 89.3%. The hyperparameters for the support vector machine are a gamma value of 0.04 and a cost value of 0.7. These values were selected by tuning the SVM with a sequence of ten different values for gamma and cost respectively. The final models were the support vector machine with a prediction accuracy of 90.96% on the test set, and the logistic regression model with a prediction accuracy of 89.3% on the test set.

The logistic regression model offered insight as to which predictors were statistically significant for the model. Those variables were Gender1 (Male), Amount_purchased, Frequency, P_Child, P_Cook, P_DIY, P_Art. The linear regression model had the same statistically significant variables as the logistic model. Of the significant variables, Gender1, Frequency, P_Child, P_Cook, and P_DIY all have negative coefficients, while Amount_purchased and P_Art have positive coefficients. This would seem to indicate that Females who purchase more, but less frequently in general and purchase art books specifically are the ideal customers to target for the sale of The Art History of Florence. This makes sense as customers whose purchase amount is high and purchase art books would most likely purchase an art history book. Another way to look at it, is that Males along with people who buy frequently in general and purchase children’s, cook and DIY books should probably be avoided when target marketing.

The next thing to look at is variable importance. This is of particular interest to the BBBC since it will help with their targeted advertising. According to the linear regression model, the three most important variables are P_Art, Gender, and Frequency, in that order. These top three variables in terms of importance are the same for the logistic regression model. Combining the variable significance and variable importance we see who the target audience should be.

The final piece of the puzzle involves looking at how much more profit, it any, would be obtained by using the targeted marketing as opposed to mailing to everybody. Using the SVM model, the profits from targeted marketing are actually less than the profits from mailing all 2300 people. The SVM predicted that 88 people would buy the book which would cost \$57.20 in postage. But of those 88 predicted only 42 people actually bought the book. So, after cost and overage for each book (\$10.20), the net profit from book sales is \$428.40. Subtract the postage and the total profit is \$371.20. In comparison, the mailing was sent to all 2300 customers, the postage would be \$1495. 204 actual people bought the book which is a net profit from book sales of \$2080.80. Subtract the postage and the total profit is \$585.80. Over $200 more than the targeted marketing.

Using the logistic model, we have better results. The logistic regression model predicted that 188 people would buy the book which would cost \$122.20. Of the 188 predicted to buy, 73 actually did. So, after cost and overage for each book, the net profit from book sales is \$744.60. Subtract the postage and the total profit is \$622.40. This is better than the $585.80 total profit from mailing all 2300 customers.

So, even though the support vector machine model achieved the highest prediction accuracy overall, the logistic regression model actually did better at accurately predicting more true positives then the SVM. The SVM’s slightly superior accuracy comes from its ability to better predict true negatives. This most likely happened because the number of people who actually bought the book in the test set is 204 out of 2300. This is 8.86%, which is much lower than the 25% who bought the book in the training set. As mentioned before, this bias could distort the results and that appears to be the case here.

# Conclusions and Recommendations

In conclusion, the support vector machine model and the logistic regression model are recommended for predicting whether or not a BBBC customer will buy the book The Art History of Florence as well as providing insight as to which type of customers should be targeted with specific advertising. The SVM model narrowly edged
out the logistic regression model by less than two percent. If it is solely about prediction, and which model has the highest accuracy, then go with the support vector machine model. But since the various factors that influence whether or not someone will buy the book are in demand, we recommend including the logistic regression. Another suggestion is for the Book Binder Book Club to develop expertise in both logistic regression and support vector machines. Since their dependent variable Choice is binary, they will be dealing with classification problems. So, it would make the most sense to develop expertise in classification models. We would recommend developing expertise in both simple classification models (like logistic regression) as well as more complex classification models.

If the main goal of the BBBC is to maximize profits, then the focus should be on more than just prediction accuracy. As demonstrated in this analysis, the model with a slightly lower overall prediction accuracy would net the company more profits then had they used the model with the higher accuracy. This is most likely due to the odd train/test split as well as the number of customers who bought the book being overrepresented in the training data.

A potential continuation of this analysis could include other well-performing classification models such as decision trees, random forests, and neural networks. These three models are at different ends of the complexity spectrum, so it would be interesting to see how they would stack up against each other and the other models presented here. We advise using a stratified random sample in the training data so that the true ratio of buyers to non-buyers is represented. This should eliminate much of the bias and lead to models that are truly representative of the data as a whole.


# Code

```{r, message=FALSE}
library(tidyverse)
library(e1071)
library(caret)
library(readxl)
library(corrplot)
```

Read in the data:
```{r}
bbbc_train <- read_excel('BBBC-Train.xlsx')
bbbc_test <- read_excel('BBBC-Test.xlsx')
bbbc_predict <- read_excel('BBBC-Predict.xlsx')
```

Convert to data.frame:
```{r}
bbbc_train <- as.data.frame(bbbc_train)
bbbc_test <- as.data.frame(bbbc_test)
bbbc_predict <- as.data.frame(bbbc_predict)
```

Look at structure of data:
```{r}
str(bbbc_train)
str(bbbc_test)
```

Check for missing values:
```{r}
anyNA(bbbc_train)
anyNA(bbbc_test)
```

Drop Observation variable:
```{r}
bbbc_train <- bbbc_train[, 2:12]
bbbc_test <- bbbc_test[, 2:12]
```

Check for near zero variance:
```{r}
zeroVar <- nearZeroVar(bbbc_train)
zeroVar
```

Correlation plot:
```{r}
bbbc_cor <- cor(bbbc_train[, 3:11])
bbbc_cor
corrplot(bbbc_cor, order = 'hclust', tl.cex = 0.8)
high_cor <- findCorrelation(bbbc_cor, cutoff = 0.8)
high_cor
```

Factor the Choice and Gender variables:
```{r}
bbbc_train$Choice <- as.factor(bbbc_train$Choice)
bbbc_train$Gender <- as.factor(bbbc_train$Gender)

bbbc_test$Choice <- as.factor(bbbc_test$Choice)
bbbc_test$Gender <- as.factor(bbbc_test$Gender)
```

Variable histograms:
```{r}
par(mfrow = c(3,3))
hist(bbbc_train$Amount_purchased, main = 'Amount Purchased')
hist(bbbc_train$Frequency, main ='Frequency')
hist(bbbc_train$Last_purchase, main = 'Last Purchase')
hist(bbbc_train$First_purchase, main = 'First Purchase')
hist(bbbc_train$P_Child, main = 'Childrens Books Purchased')
hist(bbbc_train$P_Youth, main = 'Youth Books Purchased')
hist(bbbc_train$P_Cook, main = 'Cook Books Purchased')
hist(bbbc_train$P_DIY, main = 'DIY Books Purchased')
hist(bbbc_train$P_Art, main = 'Art Books Purchased')
```

Pre-processing of the data:
```{r}
pre_process <- preProcess(bbbc_train, method = c('center', 'scale', 'BoxCox'))
pre_process
processed_bbbc_train <- predict(pre_process, newdata = bbbc_train)
processed_bbbc_test <- predict(pre_process, newdata = bbbc_test)
```

Variable histograms after transformations:
```{r}
par(mfrow = c(3,3))
hist(processed_bbbc_train$Amount_purchased, main = 'Amount Purchased')
hist(processed_bbbc_train$Frequency, main ='Frequency')
hist(processed_bbbc_train$Last_purchase, main = 'Last Purchase')
hist(processed_bbbc_train$First_purchase, main = 'First Purchase')
hist(processed_bbbc_train$P_Child, main = 'Childrens Books Purchased')
hist(processed_bbbc_train$P_Youth, main = 'Youth Books Purchased')
hist(processed_bbbc_train$P_Cook, main = 'Cook Books Purchased')
hist(processed_bbbc_train$P_DIY, main = 'DIY Books Purchased')
hist(processed_bbbc_train$P_Art, main = 'Art Books Purchased')
```

##  Logistic Regression

```{r}
set.seed(22)
log_model <- glm(Choice ~., data = processed_bbbc_train, family = 'binomial')
summary(log_model)
```
Predictions:
```{r}
# Predictions
log_pred <- predict(log_model, processed_bbbc_test, type = 'response')
log_prob <- ifelse(log_pred >= 0.5, 1, 0)
confusionMatrix(as.factor(log_prob), processed_bbbc_test$Choice)
```
Variable importance:
```{r}

log_varimp <- varImp(log_model, scale=FALSE)
log_varimp
```

## Support Vector Machine
```{r}
set.seed(22)
svm_model <- tune.svm(Choice ~., data = processed_bbbc_train, gamma = seq(0.01, 0.1, by = 0.01),
                  cost = seq(0.1, 1, by = 0.1))
```

Best parameters for the SVM:
```{r}
svm_model$best.parameters
```

Model with best parameters used:
```{r}
mysvm <- svm(Choice ~., data = processed_bbbc_train, gamma = svm_model$best.parameters$gamma, 
             cost = svm_model$best.parameters$cost)
summary(mysvm)
```

Predictions:
```{r}
svmpredict <- predict(mysvm, newdata = processed_bbbc_test, type = 'response')
confusionMatrix(as.factor(svmpredict), processed_bbbc_test$Choice)
```

## Linear Regression

Convert variable Choice to numeric:
```{r}
processed_bbbc_train$Choice <- as.numeric(processed_bbbc_train$Choice) - 1
processed_bbbc_train$Gender <- as.numeric(processed_bbbc_train$Gender) - 1

processed_bbbc_test$Choice <- as.numeric(processed_bbbc_test$Choice) - 1
processed_bbbc_test$Gender <- as.numeric(processed_bbbc_test$Gender) - 1
```

```{r}
set.seed(22)
lm_model <- lm(Choice ~., data = processed_bbbc_train)
summary(lm_model)
```

Predictions:
```{r}
lm_pred <- predict(lm_model, processed_bbbc_test, type = 'response')
```

Compute Mean Squared Error:
```{r}
mean((lm_pred - processed_bbbc_test$Choice)^2)
```

Variable importance:
```{r}
lm_varimp <- varImp(lm_model, scale=FALSE)
lm_varimp
```

