---
title: "Bank Marketing Case Study"
author: "Matthew Smith"
date: "4/6/2020"
output:
  md_document:
    variant: markdown_github
---

```{r setup, include=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Executive Summary


The model that produced the highest prediction accuracy was the logistic regression model. The model scored a 90.64% accuracy on the testing data. The random forest model was right behind with a test accuracy of 90.16%. The model with the lowest prediction accuracy was the k nearest neighbor, with a test accuracy of 89.91%. These models all performed quite well, with a total spread of less than 1% in prediction accuracy on the test dataset. The logistic model was more accurate than the random forest model and the KNN model, and also provided insight into which predictors are statistically significant. This makes it the best model on two fronts, it has the higher prediction accuracy and is the least complex model which means it is the most interpretable.

    
# The Problem


The task of this particular case study was to predict if the client of this Portuguese bank will subscribe (yes/no) to a term deposit (y). This data comes from a direct marketing campaign from a Portuguese banking institution. The data was collected from phone calls, where it often took more than one phone call to get the answer to the bank term deposit question.

Since this is a binary classification problem, the models to use for this particular problem need to be good at modeling classification problems. That is why I chose a logistic regression model, a k nearest neighbor model, and a random forest model. The logistic model was going to be used as a baseline to compare the usually more accurate random forest and KNN models. However, as it turns out after running the models, the logistic mode beat the other two more complex models. Logistic regression is the classic model in terms of classification, and in many cases, can do as well or even out-perform the more complex random forest and KNN, as seen here. Model complexity is another factor that needs to be considered when comparing these models. Since the model with highest accuracy is also the least complex model, we don’t have to give up model interpretability for prediction accuracy or vise versa. In this case we have the best of both worlds.

Our task defined in this case study is to simply predict if the client will subscribe (yes/no) to a term deposit (y). This means that our main concern is prediction accuracy, but since the least complex model is the model with the highest prediction accuracy, we also benefit from model interpretability. Therefore, the model of choice is the logistic regression model, chosen on the model’s prediction accuracy. In the following report, we will explore the methodology used, including selecting parameters for each model, as well as the data itself, which predictors were dropped and why. The report will finish with detailed findings and a conclusion.

    
# Review of Related Literature


In this analysis the logistic regression model is the model of choice because it achieved the highest prediction accuracy. It also is one of the less complex models, so interpretability is an advantage. This is not the case for the other two models performed in this analysis, the random forest and k nearest neighbor. These models are more complex and therefor lose that interpretability.

Another method that was considered for this analysis was the linear discriminate analysis (LDA), but upon review of the literature I decided against it. According to S. James Press and Sandra Wilson in their article “Choosing Between Logistic Regression and Discriminant Analysis” in the Journal of the American Statistical Association, logistic regression is preferred to discriminate analysis when there is at least one qualitative variable. In this bank dataset, there are multiple qualitative variables making it better suited for a logistic regression as opposed to an LDA. This is not to say that an LDA model would not work well, just that I chose the logistic regression model over it for the purposes of this analysis.

    
# Methodology


For this case study, I compared the results of a logistic regression model, a random forest model, and a KNN model. For the random forest model, an mtryGrid was created to pick the number of predictors that were tried at each split in the decision trees. Another parameter of the random forest was the ntree, which is the number of decision trees that make up the random forest. The final parameter was the trControl, which is a resampling method. This was set before hand in ctrl. The method used was cross-validation with 10 folds. After the 10-fold cross-validation, the final value used for the model was mtry = 10, meaning that 10 predictors were tried at each split of the 200 decision trees. For the KNN model, the parameters were the same ctrl used in the random forest model. This was done to make it comparable using resampling. Another parameter was preProcess, where the data was centered and scaled, and the final parameter was tuneLength, which was set to 10.

The data was split for training and test sets: 80/20. This dataset is sufficiently large enough to justify the 80/20 split, as opposed to something like a 70/30 or less split. The total dataset consists of 4119 observations. The training set consists 3296 observations and the test set consists of 823 observations. After running the logistic regression, the number of variables will increase due to the dummy coding of the categorical variables. The random forest and KNN models don’t do the dummy coding in the modeling but do dummy code when looking at variable importance.

    
# Data


The data was extremely clean, which was nice. The banking dataset consisted of 21 variables and 4119 observations. The data didn’t have any missing values, so there was no need to drop any observations or impute values. The categorical variables in the dataset are counted as factors in R. The rest of the variables are continuous and are represented as numeric or integers in R.

I removed two variables from the dataset, duration and education. Duration was removed because it is not conducive to a model whose main focus is prediction. This variable is how long the customer stayed on the phone, which is relevant to whether or not they subscribed, but there is no way to know how long a customer will stay on the phone beforehand. This lessens the predictor’s value in terms of prediction but makes it useful in a general analysis of the data. In fact, after running the models with duration still in the dataset, it had the highest importance ranking in all three models. This indicates that how long a person stays on the phone offers insight about whether not someone will subscribe, but it is not useful in terms of predictions.

The education variable was removed because when it came time to predict on the testing dataset using the models, there was an issue with one of the levels of education - illiterate. I could not figure out exactly what the problem was, but I believe it is possible that it had so few instances that the level only appeared in either the test or training set and not the other. This didn’t appear to have much of an effect on the testing accuracy of the models, since they were all above 90%. Also, after running a logistic regression with education included, none of the dummy variables associated with education were statistically significant.


# Findings


This analysis showed that the logistic regression model had the highest accuracy rate of all the models, with an accuracy of 90.64%. Right behind the logistic regression model was the random forest model, with an accuracy of 90.16%. The hyperparameters for this model include 200 trees and an mtry value of 10, which was picked by 10-fold cross-validation. The final model was the KNN. The hyperparameters for this model were pre-processing of the data – center and scale – and the final value for k was 17, picked by 10-fold cross-validation.

The next thing to compare between the three models was variable importance. For the logistic regression model, the variables that were most important were contacttelephone, poutcomenonexistent, monthmar, and cons_conf_idx. For the KNN model, the variables that were most important were euribor3m, nr_employed, emp_var_rate. For the random forest model, the variables that were most important were nr_employed, pdays, euribor3m. Both the random forest and KNN models had euribor3m and nr_employed as important, but the more accurate model, the logistic regression, didn’t have either. It is interesting that there is not much overlap in variable importance between the three models even though their prediction accuracies are so similar. The reasons why could be something that could be explored in a future analysis.
    
# Conclusion and Recommendations


In conclusion, the logistic regression model is recommended for predicting whether or not the clients of a Portuguese bank will subscribe to a term deposit. The logistic model narrowly edged out the random forest and KNN models by less than a percent. In reality, any of the three models could be used since their respective accuracies are so close. It would depend on what other information, if any, was wanted from the analysis. If it is solely about prediction, and which model has the highest accuracy, then go with the logistic regression model.

A potential continuation of this analysis could include other well-performing classification models such as decision trees and neural networks. These two models are at different ends of the complexity spectrum, so it would be interesting to see how they would stack up against each other and the other three models presented here. Another area that could be explored is using a variable selection method to reduce the number of variables, and to see how that reduction effects the prediction accuracy in these models.


# Code

```{r, message=FALSE}
library(tidyverse)
library(caret)
library(AppliedPredictiveModeling)
library(janitor)
library(e1071)
```

```{r}
set.seed(22)
```

```{r}
bank <- read.csv("bank-additional.csv", header = TRUE, sep = ";")
```

```{r}
str(bank)
```

```{r}
summary(bank)
```

Factor the dependent variable:
```{r}
bank$y <- as.factor(bank$y)
```

Clean up variable names:
```{r}
bank2 <- clean_names(bank, "snake")
```

Remove the variables duration and education:
```{r}
bank2 <- bank2[, -c(4, 11)]
str(bank2)
```

Create 80/20 training/test split:
```{r}
set.seed(22)
inTrain <- createDataPartition(bank2$y, p = 0.8, list = FALSE)
bank_train <- bank2[inTrain, ]
bank_test <- bank2[-inTrain, ]
```

Set the trainControl to 10-fold cross validation to be used across all three models:
```{r}
ctrl <- trainControl(method = "cv", number = 10, classProbs = TRUE)
```

## Logistic Regression
```{r, warning=FALSE}
log_fit <- train(y ~., data = bank_train,
                 method = "glm",
                 trControl = ctrl)
summary(log_fit)
```

Variable Importance:
```{r}
log_imp <- varImp(log_fit, scale = FALSE, competes = FALSE)
log_imp
```

Predictions:
```{r, warning=FALSE}
pred_log <- predict(log_fit, newdata = bank_test)
confusionMatrix(pred_log, bank_test$y)
```

## Random Forest

```{r}
mtryGrid <- data.frame(mtry = floor(seq(10, ncol(bank_train), length = 10)))
```

```{r}
set.seed(22)
rf <- train(y ~., data = bank_train,
            method = "rf",
            tuneGrid = mtryGrid,
            ntree = 200,
            importance = TRUE,
            trControl = ctrl)
rf
```

Variable importance:

```{r}
rf_imp <- varImp(rf, scale = FALSE, competes = FALSE)
rf_imp
```


Predictions:
```{r}
rf_pred <- predict(rf, newdata = bank_test)
confusionMatrix(rf_pred, bank_test$y)
```

## K Nearest Neighbor

```{r, warning=FALSE}
set.seed(22)
knn <- train(y ~., data = bank_train,
             method = "knn",
             trControl = ctrl,
             preProcess = c("center", "scale"),
             tuneLength = 10)
knn
```

Predictions:
```{r}
knn_pred <- predict(knn, newdata = bank_test)
confusionMatrix(knn_pred, bank_test$y)
```

